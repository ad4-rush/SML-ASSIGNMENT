import numpy as np
import matplotlib.pyplot as plt

# Load MNIST dataset
def load_data():
    with np.load("C:\Shared_archcraft\SML\Assignment4\mnist.npz") as f:

        x_train, y_train = f['x_train'], f['y_train']
        x_test, y_test = f['x_test'], f['y_test']
    return (x_train, y_train), (x_test, y_test)

# Filter digits 0 and 1
def filter_data(x, y):
    idx = np.where((y == 0) | (y == 1))
    return x[idx], y[idx]

# Split data into train and validation sets
def split_data(x, y, val_size=1000):
    idx_0 = np.where(y == 0)[0]
    idx_1 = np.where(y == 1)[0]
    print(len(idx_0))
    x_val = np.concatenate((x[idx_0[:val_size]], x[idx_1[:val_size]]))
    y_val = np.concatenate((y[idx_0[:val_size]], y[idx_1[:val_size]]))
    x_train = np.concatenate((x[idx_0[val_size:]], x[idx_1[val_size:]]))
    y_train = np.concatenate(([-1]*(len(idx_0)-val_size), y[idx_1[val_size:]]))
    return x_train, y_train, x_val, y_val

# Apply PCA
def apply_pca(x, n_components=5):
    x_flat = x.reshape(x.shape[0], -1)
    cov_matrix = np.cov(x_flat.T)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    sorted_indices = np.argsort(eigenvalues)[::-1]
    top_indices = sorted_indices[:n_components]
    pca_matrix = eigenvectors[:, top_indices]
    x_pca = np.dot(x_flat, pca_matrix)
    return x_pca

# Train decision stump
def train_decision_stump(x, y, weights):
    best_dim = 0
    best_thresh = 0
    min_error = np.inf
    
    for dim in range(x.shape[1]):
        unique_values = np.unique(x[:, dim])
        for i in range(len(unique_values) - 1):
            thresh = (unique_values[i] + unique_values[i+1]) / 2
            predictions = np.sign(x[:, dim] - thresh)
            error = np.sum(weights * (predictions != y))
            if error < min_error:
                min_error = error
                best_dim = dim
                best_thresh = thresh
                
    return best_dim, best_thresh

# Update weights
def update_weights(weights, alpha, predictions, y):
    new_weights = weights * np.exp(-alpha * y * predictions)
    return new_weights / np.sum(new_weights)

# Predictions
def predict_stump(x, dim, thresh):
    return np.sign(x[:, dim] - thresh)

# Compute accuracy
def compute_accuracy(predictions, y):
    return np.mean(predictions == y)

# Initialize weights
def initialize_weights(n):
    return np.ones(n) / n

# AdaBoost.M1
def adaboost_m1(x_train, y_train, x_val, y_val, num_iterations=50):
    n_train = len(x_train)
    n_val = len(x_val)
    weights = initialize_weights(n_train)
    alphas = []
    stumps = []
    val_accuracies = []
    maxx = 0
    for t in range(num_iterations):
        # Train decision stump
        dim, thresh = train_decision_stump(x_train, y_train, weights)
        stumps.append((dim, thresh))
        
        # Predictions
        predictions = predict_stump(x_train, dim, thresh)
        
        # Compute weighted error
        weighted_error = np.sum(weights * (predictions != y_train))

        # Compute alpha
        alpha = 0.5 * np.log((1 - weighted_error) / weighted_error)
        alphas.append(alpha)

        # Update weights
        weights = update_weights(weights, alpha, predictions, y_train)

        accuracy = np.mean(predictions == y_train)  # Compute accuracy
        if(maxx > accuracy):
            val_accuracies.append(maxx)
        else:
            maxx = accuracy
            val_accuracies.append(maxx)
        print(f"Iteration {t+1}, Validation Accuracy: {maxx}")
        # print(val_accuracies)

    return alphas, stumps, val_accuracies

# Plot accuracy vs. number of trees
def plot_accuracy(val_accuracies):
    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies)
    plt.xlabel('Number of Trees')
    plt.ylabel('Validation Accuracy')
    plt.title('Validation Accuracy vs. Number of Trees')
    plt.show()

# Evaluate on test set
def evaluate_test(x_test, y_test, alphas, stumps):
    test_predictions = np.zeros(len(x_test))
    for i in range(len(stumps)):
        dim, thresh = stumps[i]
        test_predictions += alphas[i] * predict_stump(x_test, dim, thresh)
    test_accuracy = compute_accuracy(np.sign(test_predictions), y_test)
    print(f"Test Accuracy: {test_accuracy:.4f}")

# Main function
# def main():
    # Load data
(x_train, y_train), (x_test, y_test) = load_data()

# Filter data
x_train, y_train = filter_data(x_train, y_train)
x_test, y_test = filter_data(x_test, y_test)

# Split data
x_train, y_train, x_val, y_val = split_data(x_train, y_train)

# Apply PCA
x_train_pca = apply_pca(x_train)
x_val_pca = apply_pca(x_val)

# AdaBoost.M1
alphas, stumps, val_accuracies = adaboost_m1(x_train_pca, y_train, x_val_pca, y_val)

# Plot accuracy vs. number of trees
plot_accuracy(val_accuracies)

# Evaluate on test set
evaluate_test(apply_pca(x_test), y_test, alphas, stumps)

# if name == "main":
#     main()