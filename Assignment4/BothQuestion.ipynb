{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz MNIST\n",
    "dataset for this question and select two digits - 0 and 1. Label them as -1 and\n",
    "1.\n",
    "\n",
    "\n",
    "Divide the train set into train and val set. Keep 1000 samples from each\n",
    "class for val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"C:\\Shared_archcraft\\SML\\Assignment4\\mnist.npz\") as data:\n",
    "    x_train, y_train = data['x_train'], data['y_train']\n",
    "    x_test, y_test = data['x_test'], data['y_test']\n",
    "\n",
    "train_mask = np.isin(y_train, [0])\n",
    "x_train_0 = x_train[train_mask]\n",
    "y_train_0 = [-1]*len(x_train_0)\n",
    "\n",
    "train_mask = np.isin(y_train, [1])\n",
    "x_train_1 = x_train[train_mask]\n",
    "y_train_1 = y_train[train_mask]\n",
    "\n",
    "train_mask = np.isin(y_train, [0, 1])\n",
    "x_train_01 = x_train[train_mask]\n",
    "y_train_01 = y_train[train_mask]\n",
    "\n",
    "print(len(y_train_0))\n",
    "print(len(y_train_1))\n",
    "\n",
    "\n",
    "x_test_1 = x_train_1[:1000]\n",
    "y_test_1 = y_train_1[:1000]\n",
    "x_test_0 = x_train_0[:1000]\n",
    "y_test_0 = y_train_0[:1000]\n",
    "\n",
    "x_train_0 = x_train_0[1000:]\n",
    "y_train_0 = y_train_0[1000:]\n",
    "x_train_1 = x_train_1[1000:]\n",
    "y_train_1 = y_train_1[1000:]\n",
    "\n",
    "x_real_train = np.concatenate([x_train_0, x_train_1], axis=0)\n",
    "y_real_train = np.concatenate([y_train_0, y_train_1], axis=0)\n",
    "x_real_test = np.concatenate([x_test_0, x_test_1], axis=0)\n",
    "y_real_test = np.concatenate([y_test_0, y_test_1], axis=0)\n",
    "\n",
    "# Reshape x_real_train\n",
    "# x_real_train = x_real_train.reshape(-1, 28, 28)\n",
    "print(y_real_train.shape)\n",
    "print(x_real_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(x):\n",
    "    X_mean = np.mean(x, axis=0)\n",
    "    X_centered = x - X_mean\n",
    "    X_centered_2d = X_centered.reshape(X_centered.shape[0], -1)  # Ensure X_centered is 2D\n",
    "    cov_matrix = np.cov(X_centered_2d, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    p = 5\n",
    "    top_indices = np.argsort(eigenvalues)[::-1][:p]\n",
    "    pca_matrix = eigenvectors[:, top_indices]\n",
    "    \n",
    "    x_reduced = np.dot(X_centered_2d, pca_matrix)\n",
    "    \n",
    "    # print(x_reduced.shape)\n",
    "    return x_reduced\n",
    "\n",
    "# pca(x_real_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(labels):\n",
    "    n_labels = len(labels)\n",
    "    if n_labels == 0:\n",
    "        return 0\n",
    "    # Shift labels to ensure all values are non-negative\n",
    "    shifted_labels = labels + 1  # Shift -1 to 0 and 1 to 2\n",
    "    counts = np.bincount(shifted_labels, minlength=3)\n",
    "    print(counts)  # Ensure array has at least 3 elements\n",
    "    probabilities = counts / n_labels\n",
    "    gini = 1 - np.sum(probabilities ** 2)\n",
    "    return gini\n",
    "\n",
    "def find_best_split(feature, labels, weight):\n",
    "    sorted_indices = np.argsort(feature)\n",
    "    sorted_weights = np.array(weight)[sorted_indices]\n",
    "    print(sorted_indices)\n",
    "    best_gini = float('inf')\n",
    "    best_split_value = None\n",
    "    for i in range(len(feature) - 1):\n",
    "        split_value = 0.5 * (feature[sorted_indices[i]] + feature[sorted_indices[i + 1]])\n",
    "        left_labels = labels[sorted_indices[:i + 1]]\n",
    "        right_labels = labels[sorted_indices[i + 1:]]\n",
    "        gini = (len(left_labels) * calculate_gini(left_labels) + len(right_labels) * calculate_gini(right_labels)) / len(labels)\n",
    "        if gini < best_gini:\n",
    "            best_gini = gini\n",
    "            best_split_value = split_value\n",
    "    return best_gini, best_split_value\n",
    "def grow_decision_tree(features, labels, weight):\n",
    "    n_features = features.shape[1]\n",
    "    best_split_dim = None\n",
    "    best_split_value = None\n",
    "    best_gini= float('inf')\n",
    "    \n",
    "    print(\"dimensions ::\", n_features)\n",
    "    for dim in range(n_features):\n",
    "        gini, split_value = find_best_split(features[:,dim], labels, weight)\n",
    "        if gini < best_gini:\n",
    "            best_gini = gini\n",
    "            best_split_dim = dim\n",
    "            best_split_value = split_value\n",
    "    \n",
    "    left_indices = features[:, best_split_dim] <= best_split_value\n",
    "    right_indices = ~left_indices\n",
    "    left_labels = labels[left_indices]\n",
    "    right_labels = labels[right_indices]\n",
    "    left_features = features[left_indices]\n",
    "    right_features = features[right_indices]\n",
    "    gini_left = calculate_gini(left_labels)\n",
    "    gini_right = calculate_gini(right_labels)\n",
    "\n",
    "    node = {\n",
    "        'split_dim': best_split_dim,\n",
    "        'split_value': best_split_value,\n",
    "        'left': {'class': np.argmax(np.bincount(left_labels + 1)) - 1},\n",
    "        'gini_left': gini_left,\n",
    "        'gini_right': gini_right,\n",
    "        'right': {'class': np.argmax(np.bincount(right_labels + 1)) - 1}\n",
    "    }\n",
    "    return node\n",
    "\n",
    "def classify_sample(sample, node):\n",
    "    # print(sample)\n",
    "    if 'class' in node:\n",
    "        return node['class']\n",
    "    elif sample[node['split_dim']] <= node['split_value']:\n",
    "        return classify_sample(sample, node['left'])\n",
    "    else:\n",
    "        return classify_sample(sample, node['right'])\n",
    "\n",
    "\n",
    "def classify_samples(samples, node):\n",
    "    predictions = []\n",
    "    # print(samples.shape)\n",
    "    for sample in samples:\n",
    "        class_prediction = classify_sample(sample, node)\n",
    "        predictions.append(class_prediction)\n",
    "    return predictions\n",
    "def find_Wrong_classfication(node, x_real_train, y_real_train, weight):\n",
    "    a = 0\n",
    "    total = 0\n",
    "    print(x_real_train.shape)\n",
    "    x = pca(x_real_train)\n",
    "    for i in range(len(x_real_train)):\n",
    "        if classify_sample(x[i], node) != y_real_train[i]:\n",
    "            a+=weight[i]\n",
    "        total += weight[i]\n",
    "    return a/total\n",
    "        \n",
    "    # if gini_left >= gini_right:\n",
    "    #     node = {\n",
    "    #         'split_dim': best_split_dim,\n",
    "    #         'split_value': best_split_value,\n",
    "    #         'left': grow_decision_tree(left_features, left_labels, max_nodes-1),\n",
    "    #         'gini_left': gini_left,\n",
    "    #         'gini_right': gini_right,\n",
    "    #         'right': {'class': np.argmax(np.bincount(right_labels))}\n",
    "    #     }\n",
    "    #     return node\n",
    "    # else:\n",
    "    #     node = {\n",
    "    #         'split_dim': best_split_dim,\n",
    "    #         'split_value': best_split_value,\n",
    "    #         'left': {'class': np.argmax(np.bincount(left_labels))},\n",
    "    #         'gini_left': gini_left,\n",
    "    #         'gini_right': gini_right,\n",
    "    #         'right': grow_decision_tree(right_features, right_labels, max_nodes-1)\n",
    "    #     }\n",
    "    #     return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight = [1/len(x_real_train)]*len(x_real_train)\n",
    "# print(\"HI\",y_real_train.shape)\n",
    "# # pca2(x_real_train)\n",
    "# # pca(x_real_train)\n",
    "# print(x_real_train.shape)\n",
    "# pcaa = pca(x_real_train)\n",
    "# node = grow_decision_tree(pcaa, y_real_train, weight)\n",
    "# print(node)\n",
    "# print(x_real_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.idx = None\n",
    "        self.iii = None\n",
    "\n",
    "    def train(self, X, y, weights):\n",
    "        num_samples = X.shape[0]  # Get the number of samples\n",
    "        num_features = np.prod(X.shape[1:])  # Calculate the total number of features after flattening\n",
    "\n",
    "        min_error = float('inf')\n",
    "\n",
    "        # Flatten each image into a 1-dimensional array\n",
    "        X_flat = X.reshape(num_samples, num_features)\n",
    "        # print(X_flat)\n",
    "        for idx in range(num_features):\n",
    "            value = np.unique(X_flat[:, idx])#sort\n",
    "            # print(value)\n",
    "            for ii in value:\n",
    "                pred = np.ones(num_samples)\n",
    "                pred[X_flat[:, idx] < ii] = -1\n",
    "                error = np.sum(weights[pred != y])\n",
    "\n",
    "                if error < min_error:\n",
    "                    min_error = error\n",
    "                    self.idx = idx\n",
    "                    self.iii = ii\n",
    "                    \n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        num_features = np.prod(X.shape[1:])  # Calculate the total number of features after flattening\n",
    "\n",
    "        # Flatten each image into a 1-dimensional array\n",
    "        X_flat = X.reshape(num_samples, num_features)\n",
    "\n",
    "        pred = np.ones(num_samples)\n",
    "        # print(\"Dim\",self.idx)\n",
    "        pred[X_flat[:, self.idx] < self.iii] = -1\n",
    "        return pred\n",
    "\n",
    "stumps = []\n",
    "alphas = []\n",
    "accuracies = []\n",
    "class AdaBoost:\n",
    "    def __init__(self, num_stumps):\n",
    "        self.num_stumps = num_stumps\n",
    "\n",
    "\n",
    "    # def train(self, X, y):\n",
    "    #     num_samples = X.shape[0]\n",
    "    #     weights = ([1]*num_samples) / num_samples\n",
    "    #     print(weights)\n",
    "\n",
    "    #     for _ in range(self.num_stumps):\n",
    "    #         st = DecisionStump()\n",
    "    #         st.train(X, y, weights)\n",
    "    #         error = np.sum(weights[st.predict(X) != y])\n",
    "    #         alpha = np.log((1 - error) / error)\n",
    "    #         stumps.append(st)\n",
    "    #         alphas.append(alpha)\n",
    "    #         weights = (weights *  np.exp(-0.5 * alpha * y * st.predict(X)))/np.sum(weights)\n",
    "\n",
    "    def trainAndPredict(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.array([1] * num_samples) / num_samples\n",
    "        # print(weights)\n",
    "\n",
    "\n",
    "        for i in range(self.num_stumps):\n",
    "            print(i)\n",
    "            st = DecisionStump()\n",
    "            st.train(X, y, weights)\n",
    "            error = np.sum(weights[st.predict(X) != y])\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            # print(alpha)\n",
    "            stumps.append(st)\n",
    "            alphas.append(alpha)\n",
    "            weights *= np.exp(-alpha * y * st.predict(X))\n",
    "            weights /= np.sum(weights)\n",
    "            pred = self.predict(X)\n",
    "            accuracy = np.mean(pred == y)  # Compute accuracy\n",
    "            accuracies.append(accuracy)\n",
    "            # print(\"Error in Boost\",i,\":\", 1 - accuracy)\n",
    "\n",
    "        return accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        pred = np.zeros(num_samples)\n",
    "        for i in range(len(stumps)):\n",
    "            pred += alphas[i] * stumps[i].predict(X)\n",
    "        # pred = alphas[-1] * stumps[-1].predict(X)\n",
    "        return np.sign(pred)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcaa = pca(x_real_train)\n",
    "pcaa2 = pca(x_real_test)\n",
    "adaboost = AdaBoost(num_stumps = 300)\n",
    "# predictions = adaboost.predict(pcaa)\n",
    "val_accuracies = adaboost.trainAndPredict(pcaa, y_real_train)\n",
    "\n",
    "# val_accuracies = adaboost.trainAndPredict(pcaa2, y_real_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_accuracies)\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Number of Trees')\n",
    "plt.show()\n",
    "n = 0\n",
    "some = 0\n",
    "predictions = np.zeros(len(x_real_test))\n",
    "\n",
    "for i in range(len(val_accuracies)):\n",
    "    if val_accuracies[i] > n:\n",
    "        n = val_accuracies[i]\n",
    "        some = i\n",
    "for i in range(some + 1):\n",
    "    # clf = stumps[i]\n",
    "    # alpha = alphas[i]\n",
    "    predictions += alphas[i] * stumps[i].predict(pcaa2)\n",
    "\n",
    "    some -= 1\n",
    "    if some < 0:\n",
    "        break\n",
    "\n",
    "predictions = np.sign(predictions)\n",
    "# print(predictions)\n",
    "accuracy = np.mean(predictions == y_real_test)\n",
    "print(\"Accuracy for test is ::\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "# Example usage\n",
    "predictions_train = adaboost.predict(pcaa)\n",
    "accuracy_train = max(val_accuracies)\n",
    "print(\"Training accuracy:\", accuracy_train) \n",
    "\n",
    "predictions_test = adaboost.predict(pcaa2)\n",
    "accuracy_test = accuracy(y_real_test, predictions_test)\n",
    "print(\"Test accuracy:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 Starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumOfStumps = 300\n",
    "# residue = y_real_train.copy()\n",
    "# residue = residue.astype(np.float64)\n",
    "# X = pca(x_real_train)\n",
    "\n",
    "NumOfStumps = 300\n",
    "residue = y_real_train.copy()\n",
    "residue = residue.astype(np.float64)\n",
    "X = pca(x_real_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.idx = None\n",
    "        self.iii = None\n",
    "        self.thresh = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        num_samples = X.shape[0]  # Get the number of samples\n",
    "        num_features = np.prod(X.shape[1:])  # Calculate the total number of features after flattening\n",
    "\n",
    "        min_error = float('inf')\n",
    "        # left = []\n",
    "        # right = []\n",
    "\n",
    "        # Flatten each image into a 1-dimensional array\n",
    "        X_flat = X.reshape(num_samples, num_features)\n",
    "        # print(X_flat)\n",
    "        for idx in range(num_features):\n",
    "            value = np.unique(X_flat[:, idx])#sort  \n",
    "            # print(value)\n",
    "            for ii in value:\n",
    "                left_indices = X[:, idx] <= ii\n",
    "                right_indices = X[:, idx] > ii\n",
    "                # print(\"left\",len(left_indices))\n",
    "                \n",
    "                left_mean = np.mean(y[left_indices])\n",
    "                right_mean = np.mean(y[right_indices])\n",
    "                ssr = np.sum((y[left_indices] - left_mean) ** 2) + np.sum((y[right_indices] - right_mean) ** 2)\n",
    "            \n",
    "            # Update best split if SSR is minimized\n",
    "                if ssr < min_error:\n",
    "                    min_error = ssr\n",
    "                    self.iii = {'dim': idx, 'split': ii, 'left_mean': left_mean, 'right_mean': right_mean, \"ssr\": ssr}\n",
    "                    self.thresh = ii\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        num_features = np.prod(X.shape[1:])  # Calculate the total number of features after flattening\n",
    "\n",
    "        # Flatten each image into a 1-dimensional array\n",
    "        X_flat = X.reshape(num_samples, num_features)\n",
    "\n",
    "        pred = np.ones(num_samples)\n",
    "        # Access the 'dim' attribute from 'self.iii' instead of 'self.idx'\n",
    "        pred[X_flat[:, self.iii['dim']] < self.thresh] = -1\n",
    "        return pred\n",
    "\n",
    "    def __calculate_loss(self,y, y_pred):\n",
    "        loss = (1/len(y)) * np.sum(np.square(y-y_pred))\n",
    "        return loss\n",
    "def update_residuals(y_train, left_indices, right_indices, left_residuals, right_residuals):\n",
    "    # residuals = np.zeros_like(y_train, dtype=float)\n",
    "    # print(y_train[right_indices])\n",
    "    # print(y_train[left_indices])\n",
    "    y_train[left_indices] = y_train[left_indices] - left_residuals\n",
    "    y_train[right_indices] = y_train[right_indices] - right_residuals\n",
    "    # print(y_train[right_indices])\n",
    "    # print(y_train[left_indices])\n",
    "    return y_train\n",
    "\n",
    "def update_residualss(residuals, left_indices, right_indices, left_mean_residual, right_mean_residual):\n",
    "    # updated_residuals = residuals.copy()  # Create a copy of residuals to avoid modifying the original array\n",
    "    residuals[left_indices] -= left_mean_residual\n",
    "    residuals[right_indices] -= right_mean_residual\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "num_trees = []\n",
    "best_model = None\n",
    "best_mse = float('inf')\n",
    "lambd = 0.01\n",
    "ssr = []\n",
    "stumps = []\n",
    "for i in range(NumOfStumps):\n",
    "    print(i)\n",
    "    st = DecisionStump()\n",
    "    # print(residue)\n",
    "    st.train(X, residue)\n",
    "    k = st.iii\n",
    "    # print(k)\n",
    "    # print(lambd * k[\"right_mean\"])\n",
    "    left_indices = X[:, k[\"dim\"]] <= k[\"split\"]\n",
    "    right_indices = ~left_indices\n",
    "    left_indexes = np.where(left_indices)[0]\n",
    "    right_indexes = np.where(right_indices)[0]\n",
    "\n",
    "    # predi = np.zeros(n)\n",
    "    # np.put(predi, left_idx, np.repeat(np.mean(yi[left_idx]), r))  # replace left side mean y\n",
    "    # np.put(predi, right_idx, np.repeat(np.mean(yi[right_idx]), n-r))  # right side mean y\n",
    "    \n",
    "    # predi = predi[:,None]  # make long vector (nx1) in compatible with y\n",
    "    # predf = predf + predi  # final prediction will be previous prediction value + new prediction of residual\n",
    "    \n",
    "    # ei = y - predf  # needed originl y here as residual always from original y    \n",
    "    # yi = ei # update yi as residual to reloop\n",
    "    stumps.append(st)\n",
    "    # print(right_indices)\n",
    "    # print(left_indexes)\n",
    "    # print(len(right_labels))\n",
    "\n",
    "    update_residualss(residue, left_indexes, right_indexes, lambd * k[\"left_mean\"], lambd * k[\"right_mean\"])\n",
    "    ssr.append(k[\"ssr\"])\n",
    "    mse_values.append(np.sum(np.square(residue))/len(X))\n",
    "    print(\"MSE::\",mse_values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(NumOfStumps), mse_values, marker='o', linestyle='-')\n",
    "plt.xlabel('Index of Stumps')\n",
    "plt.ylabel('MSE Values')\n",
    "plt.title('MSE Values vs. Index of Stumps')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_model_stumps = stumps[:NumOfStumps]\n",
    "test_pred = np.zeros(len(x_real_test))\n",
    "maxx = 100\n",
    "for s in best_model_stumps:\n",
    "    test_pred += 0.01 * s.predict(pcaa2)\n",
    "    test_mse = np.mean((y_real_test - test_pred) ** 2)\n",
    "    maxx = min(test_mse, maxx)\n",
    "\n",
    "print(\"Best Model Test MSE:\", maxx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
